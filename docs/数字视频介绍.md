## 数字视频介绍

图片的另一个属性是**分辨率**，即一个平面内像素的数量。通常表示成宽*高，例如下面这张 **4x4** 的图片。

[![image resolution](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/resolution.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/resolution.png)

图像或者视频还有一个属性是宽高比，它简单的描述了图像或像素的宽度和高度之间的比例关系。

宽高比（DAR） 像素宽高比（PAR）

[![display aspect ratio](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/DAR.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/DAR.png)

[![pixel aspect ratio](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/PAR.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/PAR.png)

> ## DVD 的 DAR 是 4:3
>
> 虽然 DVD 的实际分辨率是 704x480，但它依然保持 4:3 的宽高比，因为它有一个 10:11（704x10／480x11）的 PAR。

现在我们可以将**视频**定义为在**单位时间**内**连续的 n 帧**，这可以视作一个新的维度，n 即为帧率，若单位时间为秒，则等同于 FPS (每秒帧数 Frames Per Second)。

[![video](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/video.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/video.png)

播放一段视频每秒所需的数据量就是它的**比特率**（即常说的码率）。

> 比特率 = 宽 * 高 * 颜色深度 * 帧每秒

例如，一段每秒 30 帧，每像素 24 bits，分辨率是 480x240 的视频，如果我们不做任何压缩，它将需要 **82,944,000 比特每秒**或 82.944 Mbps (30x480x240x24)。

当**比特率**几乎恒定时称为恒定比特率（**CBR**）；但它也可以变化，称为可变比特率（**VBR**）。

>这个图形显示了一个受限的 VBR，当帧为黑色时不会花费太多的数据量。
>
>[![constrained vbr](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/vbr.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/vbr.png)

在早期，工程师们想出了一项技术能将视频的感官帧率加倍而**没有消耗额外带宽**。这项技术被称为**隔行扫描**；总的来说，它在一个时间点发送一个画面——画面用于填充屏幕的一半，而下一个时间点发送的画面用于填充屏幕的另一半。

如今的屏幕渲染大多使用**逐行扫描技术**。这是一种显示、存储、传输运动图像的方法，每帧中的所有行都会被依次绘制。



# 消除冗余

我们认识到，不对视频进行压缩是不行的；**一个单独的一小时长的视频**，分辨率为 720p 和 30fps 时将**需要 278GB\***。仅仅使用无损数据压缩算法——如 DEFLATE（被PKZIP, Gzip, 和 PNG 使用）——也无法充分减少视频所需的带宽，我们需要找到其它压缩视频的方法。

> *我们使用乘积得出这个数字 1280 x 720 x 24 x 30 x 3600 （宽，高，每像素比特数，fps 和秒数）

为此，我们可以**利用视觉特性**：和区分颜色相比，我们区分亮度要更加敏锐。**时间上的重复**：一段视频包含很多只有一点小小改变的图像。**图像内的重复**：每一帧也包含很多颜色相同或相似的区域。

## 颜色，亮度和我们的眼睛

我们的眼睛[对亮度比对颜色更敏感](http://vanseodesign.com/web-design/color-luminance/)，你可以看看下面的图片自己测试。

[![luminance vs color](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/luminance_vs_color.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/luminance_vs_color.png)

如果你看不出左图的**方块 A 和方块 B** 的颜色是**相同的**，那么好，是我们的大脑玩了一个小把戏，这让我们更多的去注意光与暗，而不是颜色。右边这里有一个使用同样颜色的连接器，那么我们（的大脑）就能轻易分辨出事实，它们是同样的颜色。

> **简单解释我们的眼睛工作的原理**
>
> [眼睛是一个复杂的器官](http://www.biologymad.com/nervoussystem/eyenotes.htm)，有许多部分组成，但我们最感兴趣的是视锥细胞(cones cells)和视杆细胞(rods cells)。眼睛有[大约1.2亿个视杆细胞和6百万个视锥细胞](https://en.wikipedia.org/wiki/Photoreceptor_cell)。
>
> **简单来说**，让我们把颜色和亮度放在眼睛的功能部位上。[视杆细胞](https://en.wikipedia.org/wiki/Rod_cell)**主要负责亮度**，而[视锥细胞](https://en.wikipedia.org/wiki/Cone_cell)**负责颜色**，有三种类型的视锥(cones)，每个都有不同的颜料，叫做：[S-视锥（蓝色），M-视锥（绿色）和L-视锥（红色）](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg)。
>
> 既然我们的视杆细胞（亮度）比视锥细胞多很多，一个合理的推断是相比颜色，我们有更好的能力去区分黑暗和光亮。
>
> [![eyes composition](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/eyes.jpg)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/eyes.jpg)

一旦我们知道我们对**亮度**（图像中的亮度）更敏感，我们就可以利用它。

### 颜色模型

我们最开始学习的[彩色图像的原理](https://github.com/leandromoreira/digital_video_introduction/blob/master/simplified-chinese/README-cn.md#%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD)使用的是 **RGB 模型**，但也有其他模型。有一种模型将亮度（光亮）和色度（颜色）分离开，它被称为 **YCbCr***。

> * 有很多种模型做同样的分离。

这个颜色模型使用 **Y** 来表示亮度，还有两种颜色通道：Cb（蓝色色度） 和 Cr（红色色度）。YCbCr 可以由 RGB 转换得来，也可以转换回 RGB。使用这个模型我们可以创建拥有完整色彩的图像，如下图。

[![ycbcr 例子](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/ycbcr.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/ycbcr.png)

### YCbCr 和 RGB 之间的转换

有人可能会问，在**不使用绿色（色度）**的情况下，我们如何表现出所有的色彩？

为了回答这个问题，我们将介绍从 RGB 到 YCbCr 的转换。我们将使用 [ITU-R 小组](https://en.wikipedia.org/wiki/ITU-R)*建议的[标准 BT.601](https://en.wikipedia.org/wiki/Rec._601) 中的系数。

第一步是计算亮度，我们将使用 ITU 建议的常量，并替换 RGB 值。

```
Y = 0.299R + 0.587G + 0.114B
```

一旦我们有了亮度后，我们就可以拆分颜色（蓝色色度和红色色度）

```
Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
```

并且我们也可以使用 YCbCr 转换回来，甚至得到绿色。

```
R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
```

> *组织和标准在数字视频领域中很常见，它们通常定义什么是标准，例如，[什么是 4K？我们应该使用什么帧率？分辨率？颜色模型？](https://en.wikipedia.org/wiki/Rec._2020)

通常，**显示屏**（监视器，电视机，屏幕等等）**仅使用 RGB 模型**，并以不同的方式来组织，看看下面这些放大效果：

[![pixel geometry](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/new_pixel_geometry.jpg)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/new_pixel_geometry.jpg)

### 色度子采样(Chroma subsampling)

一旦我们能从图像中分离出亮度和色度，我们就可以利用人类视觉系统对亮度比色度更敏感的特点，选择性地剔除信息。**色度子采样**是一种编码图像时，使**色度分辨率低于亮度**的技术。

[![ycbcr 子采样分辨率](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/ycbcr_subsampling_resolution.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/ycbcr_subsampling_resolution.png)



我们应该减少多少色度分辨率呢？已经有一些模式定义了如何处理分辨率和合并（`最终的颜色 = Y + Cb + Cr`）。

这些模式称为子采样系统，并被表示为 3 部分的比率 - `a:x:y`，其定义了色度平面的分辨率，与亮度平面上的、分辨率为 `a x 2` 的小块之间的关系。

- `a` 是水平采样参考 (通常是 4)，
- `x` 是第一行的色度样本数（相对于 a 的水平分辨率），
- `y` 是第二行的色度样本数。

> 存在的一个例外是 4:1:0，其在每个亮度平面分辨率为 4 x 4 的块内提供一个色度样本。

现代编解码器中使用的常用方案是： 4:4:4 (没有子采样)**, 4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1。

> YCbCr 4:2:0 合并
>
> 这是使用 YCbCr 4:2:0 合并的一个图像的一块，注意我们每像素只花费 12bit。
>
> [![YCbCr 4:2:0 合并](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/ycbcr_420_merge.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/ycbcr_420_merge.png)
>
> 

前面我们计算过我们需要 [278GB 去存储一个一小时长，分辨率在720p和30fps的视频文件](https://github.com/leandromoreira/digital_video_introduction/blob/master/simplified-chinese/README-cn.md#%E6%B6%88%E9%99%A4%E5%86%97%E4%BD%99)。如果我们使用 `YCbCr 4:2:0` 我们能剪掉`一半的大小（139GB）`*，但仍然不够理想。

> 我们通过将宽、高、颜色深度和 fps 相乘得出这个值。前面我们需要 24 bit，现在我们只需要 12 bit。



## 帧类型

现在我们进一步消除`时间冗余`，但在这之前让我们来确定一些基本术语。假设我们一段 30fps 的影片，这是最开始的 4 帧。

[![球 1](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/smw_background_ball_1.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/smw_background_ball_1.png) [![球 2](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/smw_background_ball_2.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/smw_background_ball_2.png) [![球 3](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/smw_background_ball_3.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/smw_background_ball_3.png) [![球 4](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/smw_background_ball_4.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/smw_background_ball_4.png)

我们可以在帧内看到**很多重复内容**，如**蓝色背景**，从 0 帧到第 3 帧它都没有变化。为了解决这个问题，我们可以将它们**抽象地分类**为三种类型的帧。

### I 帧（帧内编码，关键帧）

I 帧（可参考，关键帧，帧内编码）是一个**自足的帧**。它不依靠任何东西来渲染，I 帧与静态图片相似。第一帧通常是 I 帧，但我们将看到 I 帧被定期插入其它类型的帧之间。

[![球 1](https://github.com/leandromoreira/digital_video_introduction/raw/master/i/smw_background_ball_1.png)](https://github.com/leandromoreira/digital_video_introduction/blob/master/i/smw_background_ball_1.png)

























