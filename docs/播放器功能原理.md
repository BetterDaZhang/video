## 播放器功能原理

### 1. 多媒体播放器的主要功能

- 音视频回放
- 音频回放，要保证连续性
- 视频回放，要保证帧率稳定
- 音视频同时播放，要保证音视频同步

播放器的最最基本的功能和指标，就是音视频回放，连续性、稳定性、帧率控制、音视频同步。

### 2. 多媒体播放器的附加功能

- 媒体总时长和播放进度显示
- 播放/暂停/seek 等播放控制功能
- 音频可视化效果（波形、频谱显示）
- 视频播放截图功能
- 媒体信息获取（视频宽高、帧率、采样率等）
- 音视频流选择功能
- 音视频渲染设备和渲染方式选择
- 变速播放功能
- 软件音量控制
- 歌词显示功能
- 字幕流渲染功能
- 硬件加速解码

在一些媒体文件中，可能存在多个音频流，比如有普通话的音频流、广东话音频流、英语音频流、...，所以我们在播放这些媒体文件的时候，是可以选择具体的音频流或视频流的。多个音频流的视频还是比较常见的，多个视频流的比较罕见，但还理论上也是可以存在的。这样，我们就需要去实现“音视频流选择”这样一个功能。

在具体的操作系统平台上，音视频的**渲染**有不同的方式，比如音频可以有 waveout、directsound，视频可以有 gdi、directdraw，direct3d。视频渲染，还可以有等比例缩放、填充整个窗口，这样的选择。不同的渲染设备和渲染方式，会提供不同的兼容性、用户体验。因此实现这样的功能也是必要的。

变速播放功能是指，我们以更快的或更慢的速度去播放一个媒体文件。变速播放的原理在于，视频的帧率控制，增大或缩小帧率，以及音频在时间上的重采样。变速播放同时又受限于计算机的处理能力，如果要加快速度播放，那么必然需要计算机有更高的性能；反之降速播放，则可以节约性能。

软件音量控制，是独立于操作系统的音量控制，实际上是播放器内部，对音频解码数据的一个软件衰减或增益算法。多数时候，我们使用操作系统自身的音量控制即可，所以这是一个可选的附加功能。

字幕流显示，是一些媒体文件内部，是存在字幕流的，而且可能存在多个语种的字幕流，播放器可以根据客户的选择，在播放时渲染出对应的字幕。

### 3. 播放器的基本实现原理

读取文件 -> 解复用（demux）-> 音视频解码（decode）-> 渲染

所谓解复用/解封装，基本上就可以理解为对不同媒体文件格式（比如 avi、mp4、rmvb）的解析，并从中分离出未解码的 audio & video 数据、字幕流数据，等等。

那么我们如何解决，读文件、解复用、解码，这三大问题呢？还好我们有开源的 ffmpeg，读文件用 avio api，解复用 avformat 的相关 api，解码用 avcodec 的相关 api。

avformat_open_input 这个函数可以打开一个媒体文件（或流媒体 url）。

avformat_find_stream_info 这个函数可以查找媒体文件中的流信息，调用后我们就知道媒体中有多少音频流、视频流、字幕流，还能知道视频帧率、视频宽高、视频像素格式、音频采样率、音频声道数、视频压缩方式、音频压缩方式、... 等等各种你需要的信息。

avcodec_find_decoder avcodec_open2 avcodec_close 这三个函数，用于查找、打开、关闭音视频解码器。

av_read_frame 用于从媒体文件读入frame，实际上是读入一个AVPacket，而读入的AVPacket的类型可能是audio、video或subtitle（字幕）。这一步其实就是解复用了（demux）。

了解以上 ffmpeg 的接口函数，我大致可以想象自己如何构建一个播放器了，首先 avformat_open_input 打开一个文件，然后 avformat_find_stream_info 获取各种信息，然后根据获取的信息查找并打开对应的 decoder（解码器），然后用 av_read_frame 从打开的文件不停的读取 AVPacket，如果是音频，就用对应的音频解码器去解码，如果是视频，就用对应的视频解码器解码。解码出来的音视频，就扔给渲染器渲染。对的，基本流程就这样。

### 4. 更多的实现细节

- 是否需要多线程？
- 到底多少个线程是必须的？
- 线程间如何同步？
- 如何渲染音频
- 如何渲染视频
- 如何实现帧率控制
- 如何实现音视频同步
- 如何实现 seek 操作
- ...

### 5. 可移植性

- 使用标准 c 语言编写代码
- 使用可移植的标准功能库
- 跟平台相关的部分抽象出接口
- 抽象的可扩展的接口定义
- 尽量少的使用第三方库

### 6. 调试

ffmpeg 内部是使用的 av_log 进行 log 输出，我们可以通过 av_log_set_level 来设置需要查看的 log 信息等级。这个对我们很重要，有些时候，我们调用了 ffmpeg 的接口函数，却得到了不正确的返回值，但是我们又不知道时什么原因导致。这时候，我们可以将 log level 设置为 AV_LOG_TRACE。这样程序执行的时候，会提供更多的 log 信息，帮助我们分析和解决问题。

### 7. 架构设计

#### 7.1 播放器的组成模块

通过前面的播放器实现原理的介绍，我们可以初步总结出，一个播放器的主要核心模块：

- demux - 从输入文件分离出 audio packet 和 video packet 等等
- audio decode - 解码 audio packet
- video decode - 解码 video packet
- render - 负责 audio 和 video 的渲染
- adev - 抽象的 audio 输出设备
- vdev - 抽象的 video 输出设备



```c
player_thread() {
    while (1) {
        packet = demux();
        if (packet_is_audio(packet)) {
            decode_audio(buffer, packet);
            render_audio(buffer);
        }
        if (packet_is_video(packet)) {
            decode_video(buffer, packet);
            render_video(buffer);
        }
    }
}
```

#### 7.2 单线程设计缺陷

这样的设计，看起来还算行，至少可以把音视频解码和输出，可以说是最简单的播放器。那么缺陷在哪里？

- 很难保证音视频回放的连续性
- audio 和 video 同步问题
- 帧率稳定性和均匀性的问题

首先 demux 出来的 packet 可能就没法保证 audio 和 video 的同步，比如 demux 出的 packet 序列如下：

```
音频采样率 44100 立体声，视频帧率 30fps。音频 pts 的 timebase 为 1/44100，视频 pts 的 timebase 为 1 / 1000
（pts 为回放时间戳，timebase 可以理解为单位）
audio packet pts 0      -> audio: 0ms    -  46.4ms（计算公式 1000 * 2048 / 44100 = 46.4）
audio packet pts 2048   -> audio: 46.4ms -  92.8ms
audio packet pts 4096   -> audio: 92.8ms - 139.3ms（至此音频已经播放 139.3ms，视频还没开始）
video packet pts 0      -> video: 0ms    -  33.0ms
video packet pts 33     -> video: 33ms   -  66.0ms
video packet pts 66     -> video: 66ms   -  99.0ms
video packet pts 100    -> video: 100ms  - 133.0ms
video packet pts 132    -> video: 133ms  - 166.0ms
...
```

这样看来，demux 出来的 packet，很可能 audio 与 video 之间的 pts 差距就非常大。

demux 本质上是磁盘 IO 或者网络 IO，要么耗时要么会阻塞线程。解码是非常耗时的，而且时间是不固定的（视频解码关键帧时也许耗时更多，非关键帧相对耗时较少）。渲染也是一个耗时的操作，视频渲染需要做像素格式转换，需要往屏幕绘制，很耗时但时间相对固定；音频渲染操作，只需要把 buffer 送给音频设备，不耗时但是可能会导致线程阻塞（阻塞可以理解为广义上的耗时）...

因此我们看到的问题就是，demux、解码、渲染这些操作，都可能非常耗时。为什么耗时是问题呢？因为视频播放，对时间非常敏感，对实时性要求也非常高。我们想想，如果视频帧率是 30fps，那么每一帧的时间只有 33ms。也就是说在 33ms 内，我们必须完成一帧视频的 **demux、decoding、rendering**；或者说这个线程的 while 循环必须平均每 33ms 就执行一次。但是在这种单线程架构中，如果其中任意一个环节耗时超过了 33ms，线程就没法持续地满足视频回放的吞吐量，就是没法做到连续播放的，更不要提音视频同步了。

解决办法：对于耗时的操作，有必要建立队列进行缓冲，同时开专门的线程进行处理。这个原理说白了就是流水线原理。最初的设计，流水线上每个工序只有一个人处理，也没有缓冲（只有一个产品在线上跑），假如某个工序很耗时，那么这个工序就必然拖慢下游的时间，从而拖慢整个流水线的时间。在耗时长的工序上，增加更多的人，增加缓冲，就能提高流水线的效率。队列就是用于衔接不同工序的缓冲，线程就是工位上做事情的人。耗时的任务，cpu 就会让线程执行更多的时间，直到线程把自己的队列都填满了，就进入阻塞。

因此，要实现一个播放器，我们**必须使用多线程**，使用多线程的目的，就是为了保证视频播放的连续性、帧率稳定和音视频同步。

#### 7.3 多线程的架构设计

基于这样的原理，我们可以做出新的架构设计：

- ffplayer 负责 demux 和 decode
- render 负责音视频的渲染工作
- adev & vdev 是抽象的音视频设备
- 多线程的架构总共五个线程（demux,adecode,vdecode,arender,vrender）
- packet 队列用于 demux 和 decode
- audio buffer 队列用于 audio decode 和 audio render
- video buffer 队列用于 video decode 和 video render

总结下来，我们必须要有 5 个线程，分别用于 demux, audio decode, video decode, audio render, video render。具体实现过程中，render 的线程是 adev/vdev 的内部实现，跟平台相关。ffplayer 的 windows 版本实现中，vdev 内部是有线程的，而 adev 的实现没有用到线程，但这个线程实际上是运行在 windows 操作系统音频驱动的内部的。

所以我们也可认为只有 **3 个线程**，**demux、audio decode 和 video decode**，而 adev 和 vdev 是带缓冲队列的渲染设备。

线程之间的数据传递方式为：

- 每个线程就是一个独立的工作单元
- 线程之间通过队列传递数据
- 队列的上下游都是工作线程
- 上游的工作线程是队列的生产者
- 下游的工作线程是队列的消费者

**总结来说，就是一个源头、五个线程、三个队列、两个设备。这就是 ffplayer 的最终架构。**





### 来源

[rockcarry/ffplayer](https://github.com/rockcarry/ffplayer)

[播放器功能和原理](https://github.com/rockcarry/ffplayer/wiki/%E6%92%AD%E6%94%BE%E5%99%A8%E5%8A%9F%E8%83%BD%E5%92%8C%E5%8E%9F%E7%90%86)

[HTML5音乐可视化](https://www.imooc.com/learn/299)